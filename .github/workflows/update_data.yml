name: Update data on a schedule

on:
    schedule:
      - cron: 0 0 1 * * # Runs on the first day of every month
    workflow_dispatch: # Can be run manually if needed

jobs:
  update-data:
    runs-on: ubuntu-latest

    permissions:
      # Give the default GITHUB_TOKEN write permission to commit and push the
      # added or changed files to the repository and create a pull request
      contents: write
      pull-requests: write

    steps:
      - uses: actions/checkout@v4

      # checks if the required env variables and secrets are present
      - name: Validate required variables and secrets
        run: |
          missing_vars=()

          # Check for required variables
          [ -z "${{ secrets.GITHUB_TOKEN }}" ] && missing_vars+=("secrets.GITHUB_TOKEN")

          # If any variables are missing, print them and exit with an error
          if [ ${#missing_vars[@]} -ne 0 ]; then
            echo "Error: The following required variables are missing:"
            printf '%s\n' "${missing_vars[@]}"
            exit 1
          fi
        shell: bash

      # Downloads and extracts the files to the respective directory
      # - name: Run the data download script
      #   run: |
      #     cd ./noaa-cpfp-point
      #     ./download.sh

      # Runs the data preprocessing and aggregation on the extracted files
      # - name: Run data preprocessing and aggregation on the downloaded data
      #   run: |
      #     cd ./noaa-cpfp-point/data_processing
      #     pip install pandas geopandas shapely
      #     python3 ./main.py
      #     python3 ./convert_to_geojson.py

      # - name: ConfigureAWS Credentials 
      #   uses: aws-actions/configure-aws-credentials@v4
      #   with:
      #     role-to-assume: ${{ secrets.DEPLOYMENT_ROLE_ARN }}
      #     role-session-name: ${{ github.repository_owner}}
      #     aws-region: us-west-2

      # Upload the transformed files to a s3 location and remove the folder after that
      # - name: Upload to S3
      #   # only overwrites files that have changed, deletes file that exist in S3 but not locally
      #   run: |
      #     aws s3 sync ./noaa-cpfp-point/data/geojson s3://${{ vars.DEPLOYMENT_BUCKET }}/NOAA_features_API/ --cache-control max-age=30,must-revalidate,s-maxage=604800 --delete

      # - name : Remove local files
      #   run: |
      #     rm -rf ./noaa-cpfp-point/data

      - name: Ingest collections to Features API - dev
        id: ingest-collections
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SM2A_ADMIN_USERNAME: ${{ secrets.DEV_SM2A_ADMIN_USERNAME }} # admin
          SM2A_ADMIN_PASSWORD: ${{ secrets.DEV_SM2A_ADMIN_PASSWORD }} # G8V2a9h1
          SM2A_API_URL: ${{ vars.DEV_SM2A_API_URL }} # sm2a.dev.ghg.center
          DATASET_DAG_NAME: ${{ vars.DATASET_DAG_NAME }} 
        run: |
            export SM2A_ADMIN_USERNAME="${SM2A_ADMIN_USERNAME}"
            export SM2A_ADMIN_PASSWORD="${SM2A_ADMIN_PASSWORD}"
            export SM2A_API_URL="${SM2A_API_URL}"
            export DATASET_DAG_NAME="${DATASET_DAG_NAME}"

          pip install -r ./scripts/requirements.txt
          python3 ./scripts/ingest_collection.py ./scripts/NOAA-dag-config.json

      - name: Set current date as env variable
        run: echo "NOW=$(date +'%Y-%m-%d')" >> $GITHUB_ENV