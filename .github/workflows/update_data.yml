name: Update data on a schedule

on:
    schedule:
      - cron: 0 0 1 * * # Runs on the first day of every month
    workflow_dispatch: # Can be run manually if needed

jobs:
  update-data:
    runs-on: ubuntu-latest

    permissions:
      # Give the default GITHUB_TOKEN write permission to commit and push the
      # added or changed files to the repository and create a pull request
      contents: write
      pull-requests: write

    steps:
      - uses: actions/checkout@v4

      # checks if the required env variables and secrets are present
      - name: Validate required variables and secrets
        run: |
          missing_vars=()

          # Check for required variables
          [ -z "${{ secrets.GITHUB_TOKEN }}" ] && missing_vars+=("secrets.GITHUB_TOKEN")

          # If any variables are missing, print them and exit with an error
          if [ ${#missing_vars[@]} -ne 0 ]; then
            echo "Error: The following required variables are missing:"
            printf '%s\n' "${missing_vars[@]}"
            exit 1
          fi
        shell: bash

      # Downloads and extracts the files to the respective directory
      - name: Run the data download script
        run: |
          cd ./noaa-cpfp-point
          ./download.sh

      # Runs the data preprocessing and aggregation on the extracted files
      - name: Run data preprocessing and aggregation on the downloaded data
        run: |
          cd ./noaa-cpfp-point/data_processing
          pip install -r ./requirements.txt
          python3 ./main.py
          python3 ./convert_to_geojson.py

      - name: ConfigureAWS Credentials 
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.DEPLOYMENT_ROLE_ARN }}
          role-session-name: ${{ github.repository_owner}}
          aws-region: us-west-2

      # Upload the transformed files to a s3 location and remove the folder after that
      - name: Upload to S3
        # only overwrites files that have changed, deletes file that exist in S3 but not locally
        run: |
          aws s3 sync ./noaa-cpfp-point/data/geojson s3://${{ vars.DEPLOYMENT_BUCKET }}/NOAA_features_API/ --cache-control max-age=30,must-revalidate,s-maxage=604800 --delete
          rm -rf ./noaa-cpfp-point/data

      - name: Ingest collections to Features API - dev
        id: ingest-collections
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SM2A_ADMIN_USERNAME: ${{ secrets.DEV_SM2A_ADMIN_USERNAME }} # admin
          SM2A_ADMIN_PASSWORD: ${{ secrets.DEV_SM2A_ADMIN_PASSWORD }} # G8V2a9h1
          SM2A_API_URL: ${{ vars.DEV_SM2A_API_URL }} # sm2a.dev.ghg.center
          DATASET_DAG_NAME: ${{ vars.DATASET_DAG_NAME }} 
        run: |
          pip install -r ./scripts/requirements.txt
          pip install geopandas 
          python3 ./scripts/ingest_collection.py ./scripts/NOAA-dag-config.json

      - name: Set current date as env variable
        run: echo "NOW=$(date +'%Y-%m-%d')" >> $GITHUB_ENV

      # # Commit all changed files to a new branch back to the repository
      # - name: Commit changes to a new branch
      #   id: commit-changes
      #   uses: stefanzweifel/git-auto-commit-action@v5
      #   with:
      #       commit_message: Update data
      #       create_branch: true
      #       branch: update-data-${{ env.NOW }}
      #       # file_pattern: 'data/**'
      #       file_pattern: './noaa-cpfp-point/data/**'

      # # Create a PR from the new branch (if there was a change) to main
      # - name: Create pull request to main
      #   if: steps.commit-changes.outputs.changes_detected == 'true'
      #   run: |
      #     echo -e "ðŸ“… Scheduled data update ${{ env.NOW }}. ðŸ”” @slesaad\nAutomatically created by Github action" > msg
      #     gh pr create -H update-data-${{ env.NOW }} -B main --title 'Update data ${{ env.NOW }}' --body-file msg
      #   env:
      #       GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
